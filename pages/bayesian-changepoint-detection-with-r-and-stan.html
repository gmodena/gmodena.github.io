<!DOCTYPE html>
<html lang="en">
  <head>
    <link href='http://fonts.googleapis.com/css?family=Noticia+Text:400,700' rel='stylesheet' type='text/css' />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title> "Bayesian changepoint detection with R and Stan" | nowave.it </title>

    <link rel="stylesheet" href="../theme/css/style.css" type="text/css" />
    <link rel="stylesheet" href="../theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../theme/css/font-awesome.css" type="text/css"/>
  </head>
  <body>
    <div class=container>
<div class="header">
    <a href="../">nowave.it</a> <span class="muted"></span>
</div>

<div class=navigation>
    <ul>
    </ul>
</div>
<div class=separator></div>        
        <div class=body>
    <div class='page'>
        <h1> "Bayesian changepoint detection with R and Stan" </h1>
        <div class="highlight"><pre><span class="kp">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span>
</pre></div>


<p>In this document, I'll review a Bayesian approach to detect a single changepoint in a timeseries. I'll implement a model using <a href="http://mc-stan.org">stan</a> and show how to interpret its output in R using the <a href="http://mc-stan.org/interfaces/rstan">rstan</a> package. The code for this article can be found at <a href="https://github.com/gmodena/bayesian-changepoint">https://github.com/gmodena/bayesian-changepoint</a>.</p>
<h2>The frequentist approach</h2>
<p>Let <span class="math">\(D = {d_1, ..., d_n}\)</span> be a time series with <span class="math">\(n\)</span> <em>normally distributed</em> data points. To determine whether a change of parameters <span class="math">\(\mu\)</span> and <span class="math">\(\sigma\)</span> at point <span class="math">\(\tau\)</span> is significant, one possible approach would be to perform a likelihood test. In this setting run a hypothesis test where the null hypothesis <span class="math">\(H_0\)</span> is <em>there is no changepoint in <span class="math">\(D\)</span></em>, and the alternate hypothesis <span class="math">\(H_1\)</span> is <em>there is at most one changepoint at <span class="math">\(\tau \in D\)</span></em><sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup>.</p>
<p>More formally <span class="math">\(H_0\)</span> says that <span class="math">\(\forall n\)</span>  we have <span class="math">\(d_n \sim  \mathcal{N}(\mu, \sigma)\)</span> </p>
<p>Where <span class="math">\(H_1\)</span> says that  <span class="math">\(\forall n\)</span></p>
<div class="math">$$d_n \sim \begin{cases} \mathcal{N}(\mu_1, \sigma_1) &amp; \mbox{if } n  \lt \tau \\ \mathcal{N}(\mu_2, \sigma_2), &amp; \mbox{if } n  \geq \tau \end{cases}$$</div>
<p>Under <span class="math">\(H_0\)</span>, the log-likelihood of <span class="math">\(D\)</span> is <span class="math">\(log P(D|\mu, \sigma)\)</span>. Under <span class="math">\(H_1\)</span> the log-likelihood of <span class="math">\(D\)</span> is <span class="math">\(ll_\tau = log P(D_{1..{\tau-1}} | \mu_1, \sigma_1) + log P(D_{\tau..n} | \mu_2, \sigma_2)\)</span>.</p>
<p>We then perform a maximum likelihood estimation to find the value <span class="math">\(\tau\)</span> that maximizes <span class="math">\(ll_{\tau}\)</span>, which we use to construct a test statistic <span class="math">\(\lambda = 2 [ max_{\tau} ll_{\tau} - log P(D|\mu, \sigma)]\)</span>.</p>
<p>A changepoint has happened iff <span class="math">\(\lambda &gt; c\)</span>, for a given <span class="math">\(c\)</span>. What is a good value of <span class="math">\(c\)</span>?. As it turns out, <em>the appropriate value for this parameter c is still an open research question with several authors devising p values and other information criteria under different types of changes</em> <a href="https://www.jstatsoft.org/article/view/v058i03">changepoint: An R Package for Changepoint Analysis</a>. </p>
<h2>Bayesian changepoint detection</h2>
<p>In the Bayesian setting, we assume that the data <span class="math">\(D\)</span> is generated by some probability distribution parametrized by <span class="math">\(\Theta\)</span>. Our goal is to model <span class="math">\(P(\Theta|D)\)</span>.</p>
<p>From Bayes rule we know that <span class="math">\(P(\Theta|D) = \frac{P(D|\Theta)P(\Theta)}{P(D)}\)</span>. <span class="math">\(P(\Theta|D)\)</span> is called the <em>posterior</em> distribution, <span class="math">\(P(D|\Theta)\)</span> is the <em>likelihood</em> and <span class="math">\(P(\Theta)\)</span> is the the <em>prior</em>. The core of Bayesin statistics can be summarized as calculating <span class="math">\(posterior \propto likelihood \cdot prior\)</span>.</p>
<p>A Bayesian data analysis involves the following steps:</p>
<ol>
<li>define a prior distribution that incorporates your beliefs about the data</li>
<li>acquire some data</li>
<li>use Bayes rule to update the prior distribution given the newly acquired data and calculate the posterior distribution</li>
<li>analyse the posterior </li>
</ol>
<p>A Bayesian model to detect a single changepoint <span class="math">\(\tau\)</span>, modeled as a <em>uniformly distributed</em> discrete latent parameter, will look like this:</p>
<div class="math">$$ \tau \sim Uniform(1, N) $$</div>
<div class="math">$$ \mu_1 \sim \mathcal{N}(\mu_{\mu_1}, \sigma_{\mu_1}) $$</div>
<div class="math">$$ \sigma_1 \sim \mathcal{N}(\mu_{\sigma_1}, \sigma_{\sigma_1})$$</div>
<div class="math">$$ \mu_2 \sim \mathcal{N}(\mu_{\mu_2}, \sigma_{\mu_2}) $$</div>
<div class="math">$$ \sigma_2 \sim \mathcal{N}(\mu_{\sigma_2}, \sigma_{\sigma_2})$$</div>
<div class="math">$$ d_n \sim \begin{cases} \mathcal{N}(\mu_1, \sigma_1) &amp; \mbox{if } n  \lt \tau \\ \mathcal{N}(\mu_2, \sigma_2), &amp; \mbox{if } n  \geq \tau \end{cases}$$</div>
<p>In many practical applications, a closed form solution for the posterior is hard to derive (eg. the integral is hard, or impossible to calculate), and approximation methods such as  <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov Chain Monte Carlo</a> (MCMC) are required.</p>
<p>In the reminder of this document we'll code this model as a <code>stan</code> program, and fit the model with  MCMC to calculate the posterior probability <span class="math">\(P(\tau |D, \mu_1, \sigma_1, \mu_2, \sigma_2)\)</span>.</p>
<h3>Stan</h3>
<p><a href="http://mc-stan.org">stan</a> is a C++ library that comes with a domain specific modelling language very similar to statistical notation. Stan can help us to perform the Bayesian analysis steps, including MCMC inference (and more advanced techniques). The model that I will illustrate in the remainder of this document, is an adaptation of the <a href="https://pymc-devs.github.io/pymc/tutorial.html">Coal Mine Disaster</a> example from <a href="https://github.com/stan-dev/stan/releases/download/v2.12.0/stan-reference-2.12.0.pdf">Stan's manual</a> Section 12. The full code of this model can be found at <a href="https://gist.github.com/gmodena/0f316232aa2e9f7b6fc76b49f14bfb31">https://gist.github.com/gmodena/0f316232aa2e9f7b6fc76b49f14bfb31</a>.</p>
<p>A stan program is structured in blocks of code. Each block defines: 
1. the data
2. the parameters of our model
3. the model
4. some transformations of the model output</p>
<h3>Data</h3>
<p>In the <code>data</code> block we describe what the dataset looks like. We will be modelling a time series <code>D</code>, stored as an array of <code>N</code>, <code>real</code> valued, elements.
We require <code>D</code> to have at least one element (<code>int&lt;lower=1&gt; N</code>).</p>
<div class="highlight"><pre>data {
    int&lt;lower=1&gt; N;
    real D[N]; 
}
</pre></div>


<h3>Parameters</h3>
<p>The <code>parameter</code> block describes the sampling space.</p>
<p>In our example, we want to module two Gaussian, each with a <code>mu</code> and <code>sigma</code>. We constrain <code>sigma1</code> and <code>sigma2</code> to be positive
(<code>{markdown}&lt;lower=0&gt;</code>), so that we can stick an half-normal prior on them later on.</p>
<div class="highlight"><pre><span class="kn">parameters</span> <span class="p">{</span>
    <span class="kt">real</span> <span class="n">mu1</span><span class="p">;</span>
    <span class="kt">real</span> <span class="n">mu2</span><span class="p">;</span>

    <span class="kt">real</span><span class="o">&lt;</span><span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="o">&gt;</span> <span class="n">sigma1</span><span class="p">;</span>
    <span class="kt">real</span><span class="o">&lt;</span><span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="o">&gt;</span> <span class="n">sigma2</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>


<p>Stan does not allow for sampling from discrete distributions, so we will have to reparametrize our model and marginalise out  <span class="math">\(\tau\)</span>. Let's proceed one step at a time.</p>
<p>We know that <span class="math">\(P(D|\tau,\mu_1,\sigma_1,\mu_2,\sigma_2)=\frac{P(D,\tau,\mu_1,\sigma_1,\mu_2,\sigma_2)}{P(\tau,\mu_1,\sigma_1,\mu_2,\sigma_2)}\)</span>. It follows that the joint probability distribution factors as <span class="math">\(P(D,\tau,\mu_1,\sigma_1,\mu_2,\sigma_2)=P(D|\tau,\mu_1,\sigma_1,\mu_2,\sigma_2)P(\tau,\mu_1,\sigma_1,\mu_2,\sigma_2)\)</span>.</p>
<p>To marginalize out <span class="math">\(\tau\)</span> we'll consider a factorization into likelihood and prior as <span class="math">\(P(D,\mu_1,\sigma_1,\mu_2,\sigma_2) = P(D|\mu_1,\sigma_1,\mu_2,\sigma_2)P(\mu_1,\sigma_1,\mu_2,\sigma_2)\)</span>.</p>
<p>Then we calculate the likelihood <span class="math">\(P(D|\mu_1, \sigma_1, \mu_2, \sigma_2) = \sum_{n=1}^N P(\tau, D | \mu_1, \sigma_1, \mu_2, \sigma_2) = \sum_{\tau=1}^N P(\tau) P(D|\tau, \mu_1, \sigma_1, \mu_2, \sigma_2)=\sum_{\tau=1}^N P(\tau) \prod_{n=1}^N P(d_n|\tau, \mu_1, \sigma_1, \mu_2, \sigma_2)\)</span>. Where <span class="math">\(P(D|\tau, \mu_1, \sigma_1, \mu_2, \sigma_2)\)</span> is a product of Gaussians. </p>
<p>The <code>transformed parameters</code> block is used to process the <code>parameters</code> before calculating the posterior. In the block that follows we marginalise out <code>tau</code> and calculate <code>log_p(D | mu1, sd1, mu2, sd2)</code>. Since <code>stan</code> works in logarithmic scale, we'll have to take the sum of the log PDFs (<code>normal_lpdf</code>).</p>
<div class="highlight"><pre>// TODO: we can calculate log_p(D | mu1, sd1, mu2, sd2) in 
// linear time with dynamic programming
transformed parameters {
      vector[N] log_p;
      real mu;
      real sigma;
      log_p = rep_vector(-log(N), N);
      for (tau in 1:N)
        for (n in 1:N) {
          mu = n &lt; tau ? mu1 : mu2;
          sigma = i &lt; tau ? sigma1 : sigma2;
          log_p[tau] = log_p[tau] + normal_lpdf(D[n] | mu, sigma);
      }
}
</pre></div>


<p>The functions <code>normal_lpdf</code> (and <code>normal_lpdf</code> used in the <code>model</code> block) allows us to write a model in log scale, as required by Stan.</p>
<h3>Model</h3>
<p>In the <code>model</code> block we define the priors on <span class="math">\(\mu_1\)</span>, <span class="math">\(\mu_2\)</span>, <span class="math">\(\sigma_1\)</span>, <span class="math">\(\sigma_2\)</span>, and the log-likelihood <span class="math">\(\sum_{n} = log_p(d_n | \mu_1, \sigma_1, \mu_2, \sigma_2)\)</span>. A reasonably good default choice is to use an <em>half-normal</em> prior on the scale parameters <span class="math">\(\sigma_1, \sigma_2\)</span>(a negative scale would be ill defined!). Here I'm using large values for the scale parameter <span class="math">\(\sigma\)</span> to denote uncertantinty in the prior beliefs of the distribution.  See <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Prior Choice Recommendations</a> for an overview and best practices. If we know more about the data, we could use more certain values, that is define more appropriate priors.</p>
<div class="highlight"><pre><span class="kn">model</span> <span class="p">{</span>
    <span class="n">mu1</span> <span class="o">~</span> <span class="n">normal</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">100</span><span class="p">);</span>
    <span class="n">mu2</span> <span class="o">~</span> <span class="n">normal</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">100</span><span class="p">);</span>

    <span class="o">//</span> <span class="n">scale</span> <span class="n">parameters</span> <span class="n">need</span> <span class="n">to</span> <span class="n">be</span> <span class="err">&gt;</span> <span class="m">0</span><span class="p">;</span>
    <span class="o">//</span> <span class="n">we</span> <span class="n">constrained</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span> <span class="n">to</span> <span class="n">be</span> <span class="n">positive</span>
    <span class="o">//</span> <span class="n">so</span> <span class="n">that</span> <span class="n">stan</span> <span class="n">interprets</span> <span class="n">the</span> <span class="n">following</span> <span class="n">as</span> <span class="n">half</span><span class="o">-</span><span class="n">normal</span> <span class="n">priors</span>
    <span class="n">sigma1</span> <span class="o">~</span> <span class="n">normal</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">100</span><span class="p">);</span>
    <span class="n">sigma2</span> <span class="o">~</span> <span class="n">normal</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">100</span><span class="p">);</span>

    <span class="n">target</span> <span class="o">+</span><span class="err">=</span> <span class="n">log_sum_exp</span><span class="p">(</span><span class="n">log_p</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>


<p>What about the posterior? This is where some of Stan's magic kicks in. At each iteration of MCMC sampling after convergence, <span class="math">\(\mu_1, \sigma_1, \mu_2, \sigma_2\)</span> are drawn from <span class="math">\(P( \mu_1, \sigma_1, \mu_2, \sigma_2 | D)\)</span>, and <span class="math">\(P(\tau| \mu_1, \sigma_1, \mu_2, \sigma_2)\)</span> is calculated based on the local unnormalized value of <code>log_p</code>. As a final step <span class="math">\(P(\tau| \mu_1, \sigma_1, \mu_2, \sigma_2)\)</span> is normalized by averaging over all draws. More details on this step can be found in Stan's manual.</p>
<h3>Discrete sampling</h3>
<p>The <code>generated quantities</code> block allows for postprocessing. We can use it to draw a discrete sampling of <code>tau</code> at each iteration using the <code>categorical_rng</code> probability distribution. At each iteration we draw a value of <code>tau</code>, and later on, we'll look at the histogram of draws to determine the most likely changepoint as the most frequent <code>tau</code>.</p>
<div class="highlight"><pre>generated quantities {
    int&lt;lower=1,upper=N&gt; tau;
    // K simplex are a data type in Stan
    simplex[N] sp;
    sp = softmax(log_p);
    tau = categorical_rng(sp);
}
</pre></div>


<p>The <code>softmax</code> transform maps <code>log_p</code> to a <a href="https://en.wikipedia.org/wiki/Simplex#The_standard_simplex">K-simplex</a>, which is the parameter type expected by <code>categorical_rng</code>. The label will be the index of <code>log_p</code>.</p>
<h2>Putting it all together</h2>
<p>Let's generate some artificial data to test if the models works as expected.</p>
<div class="highlight"><pre>x1 <span class="o">&lt;-</span> rnorm<span class="p">(</span><span class="m">41</span><span class="p">,</span> mean<span class="o">=</span><span class="m">15</span><span class="p">,</span> sd<span class="o">=</span><span class="m">1.5</span><span class="p">)</span>
x2 <span class="o">&lt;-</span> rnorm<span class="p">(</span><span class="m">79</span><span class="p">,</span> mean<span class="o">=</span><span class="m">17</span><span class="p">,</span> sd<span class="o">=</span><span class="m">1.1</span><span class="p">)</span>

x <span class="o">&lt;-</span> <span class="kt">c</span><span class="p">(</span>x1<span class="p">,</span> x2<span class="p">)</span>
plot<span class="p">(</span>x<span class="p">,</span> type<span class="o">=</span><span class="s">&#39;l&#39;</span><span class="p">)</span>
</pre></div>


<p><img alt="plot of chunk bayesian-changepoint-data" src="figure/changepoint/bayesian-changepoint-data-1.png" /></p>
<p>If all goes as expected, a changepoint should be identified at <span class="math">\(\tau = 42\)</span>.</p>
<h3>The R interface for Stan</h3>
<p>I'll be using the excellent <code>rstan</code> package to fit the model and analyse its output.</p>
<div class="highlight"><pre><span class="kn">library</span><span class="p">(</span>rstan<span class="p">)</span>
</pre></div>


<div class="highlight"><pre>## Loading required package: StanHeaders
</pre></div>


<div class="highlight"><pre>## rstan (Version 2.11.1, packaged: 2016-07-28 18:19:31 UTC, GitRev: 85f7a56811da)
</pre></div>


<div class="highlight"><pre>## For execution on a local, multicore CPU with excess RAM we recommend calling
## rstan_options(auto_write = TRUE)
## options(mc.cores = parallel::detectCores())
</pre></div>


<div class="highlight"><pre>rstan<span class="o">::</span>stan_version<span class="p">()</span>
</pre></div>


<div class="highlight"><pre>## [1] &quot;2.11.0&quot;
</pre></div>


<div class="highlight"><pre>rstan_options<span class="p">(</span>auto_write <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span> <span class="c1"># cache a compiled Stan program</span>
</pre></div>


<p>The <code>stan</code> function wraps the following three steps:</p>
<ol>
<li>Translate a model in Stan code to C++ code</li>
<li>Compile the C++ code to a dynamic shared object (DSO) and load the DSO</li>
<li>Sample given some user-specified data and other settings</li>
</ol>
<p>The function returns an S4 <code>stanfit</code> object. We can use methods such as <code>print</code> and <code>plot</code> (and <code>pairs</code>) to check the fitted results.</p>
<div class="highlight"><pre>fit <span class="o">&lt;-</span> stan<span class="p">(</span>
  file <span class="o">=</span> <span class="s">&quot;changepoint.stan&quot;</span><span class="p">,</span>  
  data <span class="o">=</span> <span class="kt">list</span><span class="p">(</span>D <span class="o">=</span> x<span class="p">,</span> N<span class="o">=</span><span class="kp">length</span><span class="p">(</span>x<span class="p">)),</span>    
  chains <span class="o">=</span> <span class="m">4</span><span class="p">,</span>             
  warmup <span class="o">=</span> <span class="m">1000</span><span class="p">,</span>          
  iter <span class="o">=</span> <span class="m">10000</span><span class="p">,</span>            
  cores <span class="o">=</span> <span class="m">4</span><span class="p">,</span>              
  refresh <span class="o">=</span> <span class="m">500</span>          
  <span class="p">)</span>
</pre></div>


<p>The parameters are self explanatory. Note that the variables naming in the <code>data</code> parameter should match the <code>data</code> block in the stan code. </p>
<p><code>print</code> gives an overview of the parameters and the log posterior <code>log_p</code>. </p>
<div class="highlight"><pre><span class="kp">print</span><span class="p">(</span>fit<span class="p">)</span>
</pre></div>


<div class="highlight"><pre>## Inference for Stan model: changepoint.
## 4 chains, each with iter=10000; warmup=1000; thin=1; 
## post-warmup draws per chain=9000, total post-warmup draws=36000.
## 
##               mean se_mean    sd    2.5%     25%     50%     75%   97.5%
## mu1          14.96    0.00  0.29   14.39   14.77   14.96   15.15   15.53
## mu2          17.06    0.00  0.12   16.83   16.99   17.06   17.14   17.29
## sigma1        1.88    0.00  0.21    1.52    1.73    1.86    2.00    2.35
## sigma2        1.06    0.00  0.08    0.91    1.00    1.05    1.11    1.24
## log_p[1]   -302.81    0.11 20.87 -349.03 -315.67 -300.92 -288.05 -267.29
## log_p[2]   -304.03    0.11 20.94 -350.32 -316.94 -302.14 -289.22 -268.37
## log_p[3]   -300.87    0.11 20.34 -345.95 -313.38 -299.02 -286.48 -266.25
## log_p[4]   -300.45    0.11 20.20 -345.09 -312.88 -298.62 -286.20 -266.03
## log_p[5]   -300.60    0.11 20.14 -345.10 -313.01 -298.81 -286.38 -266.29
## log_p[6]   -300.28    0.11 20.01 -344.35 -312.61 -298.50 -286.12 -266.19
## log_p[7]   -298.62    0.11 19.67 -341.85 -310.74 -296.88 -284.68 -265.05
## log_p[8]   -299.96    0.11 19.75 -343.32 -312.10 -298.20 -285.98 -266.24
## log_p[9]   -298.33    0.11 19.41 -340.94 -310.26 -296.59 -284.56 -265.13
## log_p[10]  -299.86    0.11 19.47 -342.60 -311.84 -298.13 -286.10 -266.59
## log_p[11]  -298.33    0.11 19.15 -340.39 -310.11 -296.61 -284.78 -265.63
## log_p[12]  -299.48    0.11 19.22 -341.67 -311.32 -297.75 -285.88 -266.64
## log_p[13]  -300.98    0.11 19.24 -343.27 -312.86 -299.26 -287.36 -268.16
## log_p[14]  -294.38    0.10 18.02 -334.02 -305.47 -292.72 -281.61 -263.72
## log_p[15]  -292.19    0.10 17.59 -330.95 -302.99 -290.58 -279.71 -262.30
## log_p[16]  -290.45    0.09 17.24 -328.39 -301.02 -288.85 -278.22 -261.11
## log_p[17]  -290.61    0.09 17.19 -328.44 -301.18 -289.04 -278.43 -261.35
## log_p[18]  -288.40    0.09 16.76 -325.22 -298.70 -286.85 -276.51 -259.89
## log_p[19]  -274.69    0.08 14.20 -306.21 -283.33 -273.27 -264.57 -250.84
## log_p[20]  -262.35    0.07 11.93 -289.10 -269.60 -261.07 -253.85 -242.59
## log_p[21]  -263.52    0.07 12.00 -290.37 -270.82 -262.24 -254.98 -243.63
## log_p[22]  -261.24    0.06 11.55 -287.13 -268.23 -259.97 -253.01 -242.15
## log_p[23]  -252.67    0.05 10.00 -275.22 -258.67 -251.52 -245.55 -236.40
## log_p[24]  -250.81    0.05  9.62 -272.56 -256.57 -249.68 -243.95 -235.19
## log_p[25]  -251.87    0.05  9.68 -273.73 -257.68 -250.73 -244.96 -236.11
## log_p[26]  -253.39    0.05  9.76 -275.43 -259.28 -252.25 -246.44 -237.43
## log_p[27]  -250.69    0.05  9.24 -271.55 -256.24 -249.58 -244.10 -235.69
## log_p[28]  -248.57    0.05  8.82 -268.55 -253.84 -247.48 -242.27 -234.32
## log_p[29]  -240.10    0.04  7.28 -256.84 -244.39 -239.13 -234.87 -228.58
## log_p[30]  -239.90    0.04  7.17 -256.37 -244.12 -238.93 -234.76 -228.58
## log_p[31]  -236.46    0.04  6.53 -251.54 -240.27 -235.54 -231.75 -226.30
## log_p[32]  -236.25    0.04  6.42 -251.10 -239.99 -235.33 -231.63 -226.25
## log_p[33]  -236.54    0.04  6.39 -251.31 -240.26 -235.65 -231.94 -226.58
## log_p[34]  -237.37    0.04  6.44 -252.22 -241.13 -236.49 -232.74 -227.30
## log_p[35]  -234.04    0.03  5.82 -247.59 -237.41 -233.22 -229.83 -225.05
## log_p[36]  -233.94    0.03  5.75 -247.32 -237.24 -233.13 -229.79 -225.04
## log_p[37]  -225.71    0.02  4.28 -235.90 -228.09 -224.99 -222.59 -219.47
## log_p[38]  -221.72    0.02  3.58 -230.42 -223.68 -221.08 -219.09 -216.70
## log_p[39]  -217.47    0.02  2.88 -224.61 -218.98 -216.91 -215.36 -213.64
## log_p[40]  -205.29    0.01  1.43 -208.96 -205.97 -204.96 -204.25 -203.56
## log_p[41]  -204.04    0.01  1.41 -207.65 -204.72 -203.71 -203.01 -202.33
## log_p[42]  -203.24    0.01  1.45 -206.93 -203.94 -202.89 -202.18 -201.49
## log_p[43]  -204.11    0.01  1.46 -207.83 -204.82 -203.77 -203.05 -202.35
## log_p[44]  -205.63    0.01  1.47 -209.37 -206.34 -205.28 -204.55 -203.86
## log_p[45]  -206.09    0.01  1.52 -209.95 -206.82 -205.73 -204.98 -204.26
## log_p[46]  -205.60    0.01  1.58 -209.63 -206.36 -205.22 -204.43 -203.68
## log_p[47]  -207.02    0.01  1.64 -211.17 -207.80 -206.64 -205.82 -205.04
## log_p[48]  -207.38    0.01  1.72 -211.72 -208.20 -206.98 -206.12 -205.29
## log_p[49]  -208.83    0.01  1.85 -213.52 -209.71 -208.40 -207.47 -206.58
## log_p[50]  -209.63    0.01  1.94 -214.53 -210.56 -209.18 -208.20 -207.26
## log_p[51]  -211.13    0.01  2.09 -216.46 -212.13 -210.64 -209.59 -208.56
## log_p[52]  -212.50    0.01  2.24 -218.23 -213.59 -211.99 -210.86 -209.73
## log_p[53]  -212.89    0.01  2.32 -218.81 -214.02 -212.37 -211.19 -210.00
## log_p[54]  -214.29    0.02  2.61 -220.91 -215.57 -213.72 -212.37 -211.01
## log_p[55]  -215.79    0.02  2.82 -222.92 -217.18 -215.18 -213.72 -212.20
## log_p[56]  -217.03    0.02  2.98 -224.55 -218.52 -216.39 -214.85 -213.20
## log_p[57]  -218.39    0.02  3.16 -226.31 -219.98 -217.72 -216.06 -214.27
## log_p[58]  -219.89    0.02  3.40 -228.36 -221.63 -219.19 -217.40 -215.40
## log_p[59]  -221.13    0.02  3.57 -229.99 -222.97 -220.42 -218.53 -216.36
## log_p[60]  -216.84    0.02  3.52 -225.59 -218.64 -216.14 -214.28 -212.16
## log_p[61]  -218.20    0.02  3.69 -227.35 -220.10 -217.47 -215.51 -213.25
## log_p[62]  -219.06    0.02  3.80 -228.46 -221.03 -218.32 -216.29 -213.93
## log_p[63]  -220.37    0.02  3.97 -230.12 -222.44 -219.60 -217.47 -214.98
## log_p[64]  -221.85    0.02  4.20 -232.13 -224.05 -221.05 -218.79 -216.11
## log_p[65]  -223.31    0.03  4.54 -234.47 -225.71 -222.47 -220.00 -217.05
## log_p[66]  -223.77    0.03  4.62 -235.09 -226.24 -222.93 -220.40 -217.37
## log_p[67]  -225.27    0.03  4.97 -237.40 -227.91 -224.39 -221.65 -218.32
## log_p[68]  -226.65    0.03  5.17 -239.24 -229.41 -225.75 -222.89 -219.36
## log_p[69]  -228.18    0.03  5.48 -241.47 -231.11 -227.24 -224.19 -220.39
## log_p[70]  -229.72    0.03  5.77 -243.71 -232.81 -228.74 -225.51 -221.44
## log_p[71]  -231.23    0.03  6.04 -245.88 -234.46 -230.22 -226.82 -222.52
## log_p[72]  -231.25    0.03  6.07 -245.92 -234.53 -230.25 -226.84 -222.48
## log_p[73]  -232.37    0.03  6.22 -247.36 -235.73 -231.34 -227.85 -223.36
## log_p[74]  -233.86    0.04  6.47 -249.44 -237.36 -232.81 -229.16 -224.43
## log_p[75]  -234.02    0.04  6.52 -249.68 -237.56 -232.96 -229.29 -224.51
## log_p[76]  -234.70    0.04  6.61 -250.57 -238.31 -233.64 -229.92 -225.03
## log_p[77]  -236.17    0.04  6.86 -252.63 -239.92 -235.10 -231.22 -226.09
## log_p[78]  -237.69    0.04  7.13 -254.77 -241.60 -236.57 -232.55 -227.17
## log_p[79]  -239.13    0.04  7.36 -256.75 -243.16 -238.00 -233.82 -228.22
## log_p[80]  -239.38    0.04  7.42 -257.06 -243.46 -238.26 -234.04 -228.37
## log_p[81]  -239.33    0.04  7.45 -257.11 -243.44 -238.20 -233.95 -228.23
## log_p[82]  -240.75    0.04  7.85 -259.48 -245.07 -239.58 -235.11 -229.06
## log_p[83]  -242.10    0.04  8.05 -261.24 -246.54 -240.90 -236.31 -230.08
## log_p[84]  -243.34    0.04  8.22 -262.89 -247.88 -242.14 -237.42 -231.00
## log_p[85]  -244.43    0.05  8.37 -264.26 -249.07 -243.22 -238.42 -231.82
## log_p[86]  -244.22    0.05  8.40 -264.11 -248.90 -243.02 -238.19 -231.55
## log_p[87]  -245.71    0.05  8.65 -266.16 -250.51 -244.47 -239.50 -232.62
## log_p[88]  -246.72    0.05  8.79 -267.43 -251.63 -245.48 -240.40 -233.38
## log_p[89]  -247.76    0.05  8.93 -268.80 -252.75 -246.52 -241.34 -234.16
## log_p[90]  -249.29    0.05  9.23 -271.00 -254.45 -248.01 -242.67 -235.22
## log_p[91]  -250.82    0.05  9.52 -273.19 -256.13 -249.50 -244.00 -236.29
## log_p[92]  -252.28    0.05  9.90 -275.55 -257.80 -250.93 -245.19 -237.14
## log_p[93]  -253.04    0.05 10.00 -276.54 -258.62 -251.68 -245.90 -237.71
## log_p[94]  -254.53    0.06 10.26 -278.63 -260.28 -253.14 -247.20 -238.76
## log_p[95]  -256.00    0.06 10.64 -280.98 -261.95 -254.60 -248.39 -239.56
## log_p[96]  -255.93    0.06 10.67 -280.96 -261.89 -254.51 -248.31 -239.51
## log_p[97]  -256.22    0.06 10.73 -281.38 -262.22 -254.79 -248.57 -239.68
## log_p[98]  -256.11    0.06 10.76 -281.34 -262.17 -254.70 -248.43 -239.54
## log_p[99]  -255.45    0.06 10.76 -280.69 -261.52 -254.02 -247.76 -238.85
## log_p[100] -256.69    0.06 10.94 -282.28 -262.86 -255.24 -248.87 -239.80
## log_p[101] -258.18    0.06 11.20 -284.38 -264.50 -256.72 -250.18 -240.88
## log_p[102] -259.70    0.06 11.54 -286.67 -266.19 -258.20 -251.47 -241.82
## log_p[103] -261.23    0.06 11.86 -289.00 -267.89 -259.71 -252.77 -242.80
## log_p[104] -261.32    0.06 11.90 -289.13 -268.03 -259.78 -252.82 -242.83
## log_p[105] -262.57    0.07 12.37 -291.49 -269.51 -261.00 -253.71 -243.35
## log_p[106] -263.11    0.07 12.45 -292.19 -270.10 -261.51 -254.20 -243.75
## log_p[107] -264.36    0.07 12.63 -293.81 -271.46 -262.77 -255.34 -244.67
## log_p[108] -265.17    0.07 12.74 -294.86 -272.33 -263.57 -256.08 -245.29
## log_p[109] -266.26    0.07 12.89 -296.29 -273.53 -264.66 -257.07 -246.13
## log_p[110] -267.57    0.07 13.08 -298.02 -274.96 -265.92 -258.25 -247.09
## log_p[111] -268.83    0.07 13.27 -299.65 -276.35 -267.17 -259.39 -248.04
## log_p[112] -270.00    0.07 13.43 -301.16 -277.60 -268.32 -260.45 -248.90
## log_p[113] -271.25    0.07 13.62 -302.79 -279.01 -269.56 -261.59 -249.85
## log_p[114] -272.00    0.07 13.72 -303.76 -279.83 -270.30 -262.25 -250.41
## log_p[115] -272.72    0.08 13.83 -304.77 -280.61 -271.03 -262.90 -250.96
## log_p[116] -271.69    0.08 13.81 -303.62 -279.57 -269.99 -261.86 -249.93
## log_p[117] -272.54    0.08 13.93 -304.78 -280.50 -270.84 -262.63 -250.58
## log_p[118] -273.25    0.08 14.04 -305.71 -281.30 -271.55 -263.30 -251.11
## log_p[119] -273.68    0.08 14.64 -307.71 -282.01 -271.90 -263.25 -250.62
## log_p[120] -273.19    0.08 14.65 -307.26 -281.54 -271.42 -262.78 -250.14
## mu           17.06    0.00  0.12   16.83   16.99   17.06   17.14   17.29
## sigma         1.06    0.00  0.08    0.91    1.00    1.05    1.11    1.24
## tau          42.35    0.01  1.58   40.00   41.00   42.00   43.00   46.00
## lp__       -204.30    0.01  1.38 -207.82 -204.97 -203.97 -203.30 -202.64
##            n_eff Rhat
## mu1        32412    1
## mu2        33381    1
## sigma1     30766    1
## sigma2     31256    1
## log_p[1]   33219    1
## log_p[2]   33226    1
## log_p[3]   33221    1
## log_p[4]   33213    1
## log_p[5]   33207    1
## log_p[6]   33198    1
## log_p[7]   33188    1
## log_p[8]   33200    1
## log_p[9]   33189    1
## log_p[10]  33219    1
## log_p[11]  33208    1
## log_p[12]  33214    1
## log_p[13]  33257    1
## log_p[14]  33270    1
## log_p[15]  33260    1
## log_p[16]  33248    1
## log_p[17]  33239    1
## log_p[18]  33227    1
## log_p[19]  33307    1
## log_p[20]  33384    1
## log_p[21]  33396    1
## log_p[22]  33376    1
## log_p[23]  33403    1
## log_p[24]  33371    1
## log_p[25]  33380    1
## log_p[26]  33440    1
## log_p[27]  33406    1
## log_p[28]  33363    1
## log_p[29]  33322    1
## log_p[30]  33277    1
## log_p[31]  33157    1
## log_p[32]  33094    1
## log_p[33]  33055    1
## log_p[34]  33054    1
## log_p[35]  32855    1
## log_p[36]  32774    1
## log_p[37]  32029    1
## log_p[38]  31102    1
## log_p[39]  29393    1
## log_p[40]  18902    1
## log_p[41]  18044    1
## log_p[42]  17955    1
## log_p[43]  18091    1
## log_p[44]  18602    1
## log_p[45]  18959    1
## log_p[46]  19281    1
## log_p[47]  20100    1
## log_p[48]  20650    1
## log_p[49]  22295    1
## log_p[50]  22928    1
## log_p[51]  24266    1
## log_p[52]  25344    1
## log_p[53]  25627    1
## log_p[54]  27340    1
## log_p[55]  28317    1
## log_p[56]  28882    1
## log_p[57]  29478    1
## log_p[58]  30167    1
## log_p[59]  30541    1
## log_p[60]  29702    1
## log_p[61]  30130    1
## log_p[62]  30306    1
## log_p[63]  30640    1
## log_p[64]  31050    1
## log_p[65]  31594    1
## log_p[66]  31601    1
## log_p[67]  31996    1
## log_p[68]  32191    1
## log_p[69]  32450    1
## log_p[70]  32661    1
## log_p[71]  32827    1
## log_p[72]  32774    1
## log_p[73]  32848    1
## log_p[74]  32981    1
## log_p[75]  32944    1
## log_p[76]  32957    1
## log_p[77]  33072    1
## log_p[78]  33185    1
## log_p[79]  33271    1
## log_p[80]  33244    1
## log_p[81]  33193    1
## log_p[82]  33311    1
## log_p[83]  33371    1
## log_p[84]  33417    1
## log_p[85]  33448    1
## log_p[86]  33396    1
## log_p[87]  33462    1
## log_p[88]  33484    1
## log_p[89]  33504    1
## log_p[90]  33572    1
## log_p[91]  33628    1
## log_p[92]  33685    1
## log_p[93]  33688    1
## log_p[94]  33729    1
## log_p[95]  33774    1
## log_p[96]  33742    1
## log_p[97]  33725    1
## log_p[98]  33691    1
## log_p[99]  33636    1
## log_p[100] 33661    1
## log_p[101] 33698    1
## log_p[102] 33739    1
## log_p[103] 33775    1
## log_p[104] 33753    1
## log_p[105] 33782    1
## log_p[106] 33776    1
## log_p[107] 33794    1
## log_p[108] 33797    1
## log_p[109] 33808    1
## log_p[110] 33826    1
## log_p[111] 33842    1
## log_p[112] 33851    1
## log_p[113] 33861    1
## log_p[114] 33853    1
## log_p[115] 33845    1
## log_p[116] 33795    1
## log_p[117] 33791    1
## log_p[118] 33783    1
## log_p[119] 33824    1
## log_p[120] 33793    1
## mu         33381    1
## sigma      31256    1
## tau        34832    1
## lp__       18036    1
## 
## Samples were drawn using NUTS(diag_e) at Wed Sep  7 11:05:57 2016.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).
</pre></div>


<p>Looking at the table, we can see that the <code>log_p</code> has a minimum at index 42. We can see this more explicitly if we look at a histogram of
the discrete values of <code>tau</code></p>
<div class="highlight"><pre>qplot<span class="p">(</span>extract<span class="p">(</span>fit<span class="p">)</span><span class="o">$</span>tau<span class="p">,</span> geom<span class="o">=</span><span class="s">&#39;histogram&#39;</span><span class="p">,</span> xlab <span class="o">=</span> <span class="s">&quot;tau&quot;</span><span class="p">)</span> 
</pre></div>


<div class="highlight"><pre>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
</pre></div>


<p><img alt="plot of chunk bayesian-changepoint-tau-hist" src="figure/changepoint/bayesian-changepoint-tau-hist-1.png" /></p>
<div class="highlight"><pre><span class="kp">print</span><span class="p">(</span>fit<span class="p">,</span> pars<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="s">&quot;tau&quot;</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre>## Inference for Stan model: changepoint.
## 4 chains, each with iter=10000; warmup=1000; thin=1; 
## post-warmup draws per chain=9000, total post-warmup draws=36000.
## 
##      mean se_mean   sd 2.5% 25% 50% 75% 97.5% n_eff Rhat
## tau 42.35    0.01 1.58   40  41  42  43    46 34832    1
## 
## Samples were drawn using NUTS(diag_e) at Wed Sep  7 11:05:57 2016.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).
</pre></div>


<p>We also get the credible inteterval of <span class="math">\(\tau\)</span> using the <code>plot</code> function with</p>
<div class="highlight"><pre>plot<span class="p">(</span>fit<span class="p">,</span> pars<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="s">&quot;tau&quot;</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre>## ci_level: 0.8 (80% intervals)
</pre></div>


<div class="highlight"><pre>## outer_level: 0.95 (95% intervals)
</pre></div>


<p><img alt="plot of chunk bayesian-changepoint-ci" src="figure/changepoint/bayesian-changepoint-ci-1.png" />
The plot indicates the expected value of <span class="math">\(\tau = 42\)</span>. The red bands denote the 80% credible interval; if we were to repeat the expriment several time, we'd expect <span class="math">\(\tau\)</span> to lie in the interval <span class="math">\(41 \leq \tau \leq 44\)</span> with probability <span class="math">\(P=0.80\)</span>. The black bands denote the 95% credible interval.</p>
<p>This result is consistent with the dataset <span class="math">\(D\)</span>.</p>
<h2>Conclusion</h2>
<p>In this document I showed a simple yet powerful bayesian model for detecting a single changepoint in a timeseries. The advantages over the frequentist approach are twofold. On the one hand, the Bayesian model gives a distribution of changepoints and not a single point estimate. This lets us reason about how credible the model output is. On the other hand, we don't need to hard-code threshold values for a test statistic, but rather embed our prior knowledge about the data to "tune" the model.</p>
<p>It's not all roses, though. The flexibility given to us by the Bayesian framework comes at the expenses of some math being needed to work with a discrete parameter. This is largely due to <code>stan</code>s implementation choices wrt discrete random variables. For a more concise changepoint model, written with the PyMC framework one can have a look at the work of my colleague Vincent at <a href="http://koaning.io/switching-to-sampling-in-order-to-switch.html">Switching to Sampling in order to Switch</a>. A drawback of the Bayesian approach is that it is relatively computationally expensive. On my machine (a 2015 retina MacBook pro), it takes about 7 seconds to run the MCMC simulation on four cores<sup id="fnref:2"><a class="footnote-ref" href="#fn:2" rel="footnote">2</a></sup>. For comparison, the <code>changepoint</code> package has a sub-second run time.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>The excellent <a href="https://cran.r-project.org/web/packages/changepoint/index.html">changepoint</a> R package implements this likelihood test, as well as its natural extension to the case of multiple change points.&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>The <code>transformed parameters</code> block has a quadratic time complexity. Stan's manual contains a dynamic programming alternative with linear time complexity.&#160;<a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript'; 
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
        </div>
        
<div class=footer>
   <div class="contact">
      <p>
        &raquo;
        <a href="mailto:gm@nowave.it">gm@nowave.it</a> |
        <a href="http://github.com/gmodena" target="_blank">github.com/gmodena</a> |
        <a href="http://twitter.com/gabriele_modena" target="_blank">http://twitter.com/gabriele_modena</a>
        &laquo;
      </p>
    </div>
  <p>&copy; Copyright <script language="JavaScript">var date = new Date(); document.write(date.getFullYear());</script> by Gabriele Modena</p>
  <p>  
    <a href="https://github.com/fjavieralba/flasky">Theme</a>  by <a href="http://fjavieralba.com">fjavieralba</a> (Modified)
  </p> 
<!--
  <p>
    <div class=social style="font-size: 27px;">
      <ul>
        <script language="JavaScript">
          u = '';
          s = '';
          document.write('<a href=\"mailto:' + u + '@' + s + '\" target=\"_blank\">');
        </script>
            <li><i class="icon-envelope icon-large"></i> </li>
        </a>
        <a href="http://twitter.com/" target="_blank"> <li> <i class="icon-twitter-sign icon-large"> </li></i> </a>
        <a href="" target="_blank"><li><i class="icon-linkedin-sign icon-large" ></i></li></a>
        <a href="" target="_blank"> <li> <i class="icon-github-sign icon-large"></i> </li> </a>
        <a href="" target="_blank"><li> <i class="icon-facebook-sign icon-large"></i></li> </a>
        <a href="" target="_blank"><li><i class="icon-google-plus-sign icon-large"></i> </li> </a>
        <a href="" target="_blank"><li><i class="icon-pinterest-sign icon-large"></i></li></a>


        <a href="../feeds/all.rss.xml" rel="alternate" title="Recent Blog Posts"><li> <i class="icon-rss icon-large"></i> </li></a>
      </ul>
    </div>
  </p>
-->
</div>    </div>
  </body>
</html>
